"""
Initialize job Lambda function.
Updates job status to 'initialized' and passes context through the workflow.

Note: job_id is now generated by start_workflow Lambda (before Step Functions starts)
so that API can return job_id immediately to frontend for polling.
"""

import json
import boto3
import logging
import os
from datetime import datetime

# Import shared progress tracker
try:
    from shared.progress_tracker import update_progress
except ImportError:
    # Fallback for local testing
    update_progress = None

logger = logging.getLogger()
logger.setLevel(logging.INFO)

dynamodb = boto3.resource('dynamodb')

def lambda_handler(event, context):
    """
    Initialize workflow processing for document review.
    
    The job_id and DynamoDB record are already created by start_workflow Lambda.
    This step updates the status and passes context to downstream steps.
    
    Args:
        event: Lambda event with job_id, timestamp, session_id, user_id, document_s3_key
        context: Lambda context
        
    Returns:
        Full context for downstream functions
    """
    try:
        # Get values from start_workflow (already in event)
        job_id = event.get('job_id')
        timestamp = event.get('timestamp')
        session_id = event.get('session_id')
        user_id = event.get('user_id')
        document_s3_key = event.get('document_s3_key')
        bucket_type = event.get('bucket_type', 'agent_processing')
        terms_profile = event.get('terms_profile', 'it_terms_updated')
        
        if not job_id or not session_id or not user_id:
            raise ValueError("job_id, session_id, and user_id are required")
        
        logger.info(f"Initializing workflow for job: {job_id}")
        
        # Update progress using shared tracker
        if update_progress and timestamp:
            update_progress(
                job_id, timestamp, 'initialized',
                'Workflow initialized, preparing document...',
                session_id=session_id,
                user_id=user_id
            )
        else:
            # Fallback: direct DynamoDB update
            table_name = os.environ.get('ANALYSES_TABLE_NAME')
            if table_name and timestamp:
                table = dynamodb.Table(table_name)
                table.update_item(
                    Key={
                        'analysis_id': job_id,
                        'timestamp': timestamp
                    },
                    UpdateExpression='SET #status = :status, stage = :stage, progress = :progress, updated_at = :updated',
                    ExpressionAttributeNames={'#status': 'status'},
                    ExpressionAttributeValues={
                        ':status': 'processing',
                        ':stage': 'initialized',
                        ':progress': 5,
                        ':updated': datetime.utcnow().isoformat()
                    }
                )
        
        logger.info(f"Job {job_id} initialized successfully")
        
        # Map bucket_type to actual bucket name from environment
        bucket_map = {
            'agent_processing': os.environ.get('AGENT_PROCESSING_BUCKET'),
            'user_documents': os.environ.get('USER_DOCUMENTS_BUCKET'),
            'knowledge': os.environ.get('KNOWLEDGE_BUCKET')
        }
        bucket_name = bucket_map.get(bucket_type, os.environ.get('AGENT_PROCESSING_BUCKET'))
        
        if not bucket_name:
            raise ValueError(f"Could not resolve bucket name for bucket_type: {bucket_type}")
        
        logger.info(f"Resolved bucket_type '{bucket_type}' to bucket_name '{bucket_name}'")
        
        # Get knowledge_base_id and region from environment or event
        # These should be passed from start_workflow Lambda
        knowledge_base_id = event.get('knowledge_base_id') or os.environ.get('KNOWLEDGE_BASE_ID')
        region = event.get('region') or os.environ.get('REGION', 'us-east-1')
        
        # Return full context for downstream functions
        return {
            "job_id": job_id,
            "timestamp": timestamp,
            "session_id": session_id,
            "user_id": user_id,
            "document_s3_key": document_s3_key,
            "bucket_type": bucket_type,
            "bucket_name": bucket_name,  # Actual S3 bucket name
            "terms_profile": terms_profile,
            "knowledge_base_id": knowledge_base_id,  # Pass through for KB queries
            "region": region,  # Pass through for KB queries
            "status": "processing"
        }
        
    except Exception as e:
        logger.error(f"Error initializing job: {e}")
        raise

