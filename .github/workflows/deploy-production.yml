name: Deploy to Production

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: us-east-1

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Build React frontend
      run: |
        cd one_l/user_interface/
        npm install
        npm run build
        echo "React frontend built successfully"

    - name: Build Lambda deployment package
      run: |
        # Configuration
        FUNCTION_NAME="document-review"
        DOCKER_IMAGE="one-l-lambda-builder"
        OUTPUT_DIR="./build"
        
        echo "Building Lambda deployment package for ${FUNCTION_NAME}..."
        
        # Create output directory
        mkdir -p ${OUTPUT_DIR}
        
        # Get commit hash for cache-busting to ensure fresh builds
        COMMIT_HASH=$(git rev-parse HEAD)
        echo "Building with commit hash: ${COMMIT_HASH} for cache-busting"
        
        # Build Docker image (force x86_64 platform for Lambda compatibility)
        # Use --build-arg with commit hash to bust cache and ensure code updates are included
        echo "Building Docker image with Lambda-compatible environment..."
        docker build --platform linux/amd64 --build-arg CACHE_BUST=${COMMIT_HASH} -t ${DOCKER_IMAGE} -f Dockerfile.lambda .
        
        # Run container to build deployment package
        echo "Creating deployment package in Lambda-compatible environment..."
        docker run --rm \
            -v $(pwd)/${OUTPUT_DIR}:/output \
            ${DOCKER_IMAGE}
        
        # Check if package was created
        if [ -f "${OUTPUT_DIR}/lambda-deployment.zip" ]; then
            echo "Deployment package created successfully"
            echo "Location: ${OUTPUT_DIR}/lambda-deployment.zip"
            echo "Package size: $(du -h ${OUTPUT_DIR}/lambda-deployment.zip | cut -f1)"
            echo "Package contents (sample):"
            unzip -l ${OUTPUT_DIR}/lambda-deployment.zip | head -20
        else
            echo "Failed to create deployment package"
            exit 1
        fi

    - name: Set up Python and install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        npm install -g aws-cdk
        sudo apt-get update && sudo apt-get install -y jq

    # Skip CDK Deploy to avoid Cognito conflicts - only updating Lambda code
    # - name: CDK Diff
    #   run: |
    #     cdk diff
    # 
    # - name: CDK Synth
    #   run: |
    #     cdk synth
    # 
    # - name: CDK Deploy
    #   run: |
    #     cdk deploy --require-approval never
    
    - name: Ensure DynamoDB sessions table exists
      run: |
        echo "Checking if sessions table exists..."
        # This step creates the sessions table if it doesn't exist
        SESSIONS_TABLE="OneL-v2-sessions"
        
        # Check if table exists
        TABLE_EXISTS=$(aws dynamodb describe-table --table-name "$SESSIONS_TABLE" --query 'Table.TableStatus' --output text 2>/dev/null || echo "NOT_FOUND")
        
        if [ "$TABLE_EXISTS" = "NOT_FOUND" ] || [ "$TABLE_EXISTS" = "None" ]; then
          echo "Sessions table does not exist. Creating it..."
          
          # Create the sessions table
          aws dynamodb create-table \
            --table-name "$SESSIONS_TABLE" \
            --attribute-definitions \
              AttributeName=session_id,AttributeType=S \
              AttributeName=user_id,AttributeType=S \
              AttributeName=created_at,AttributeType=S \
            --key-schema \
              AttributeName=session_id,KeyType=HASH \
              AttributeName=user_id,KeyType=RANGE \
            --billing-mode PAY_PER_REQUEST \
            --global-secondary-indexes \
              "[{
                \"IndexName\": \"user-index\",
                \"KeySchema\": [
                  {\"AttributeName\": \"user_id\", \"KeyType\": \"HASH\"},
                  {\"AttributeName\": \"created_at\", \"KeyType\": \"RANGE\"}
                ],
                \"Projection\": {\"ProjectionType\": \"ALL\"}
              }]"
          
          echo "Waiting for table to be created..."
          aws dynamodb wait table-exists --table-name "$SESSIONS_TABLE"
          echo "Sessions table created successfully"
        else
          echo "Sessions table already exists (status: $TABLE_EXISTS)"
        fi
    
    - name: Update Lambda functions
      run: |
        echo "Updating Lambda function code only (skipping infrastructure changes)..."
        
        # Update document-review Lambda function
        DOC_REVIEW_FUNCTION=$(aws lambda list-functions --query 'Functions[?contains(FunctionName, `OneL-v2`) && contains(FunctionName, `document-review`)].FunctionName' --output text)
        
        if [ -n "$DOC_REVIEW_FUNCTION" ]; then
          echo "Updating Lambda function: $DOC_REVIEW_FUNCTION"
          aws lambda update-function-code \
            --function-name "$DOC_REVIEW_FUNCTION" \
            --zip-file fileb://build/lambda-deployment.zip
          
          echo "Waiting for Lambda update to complete..."
          aws lambda wait function-updated --function-name "$DOC_REVIEW_FUNCTION"
          echo "Document-review Lambda function updated successfully"
        else
          echo "No document-review Lambda function found matching pattern"
        fi
        
        # Update session-management Lambda function
        SESSION_MGMT_FUNCTION=$(aws lambda list-functions --query 'Functions[?contains(FunctionName, `OneL-v2`) && contains(FunctionName, `session-management`)].FunctionName' --output text)
        
        if [ -n "$SESSION_MGMT_FUNCTION" ]; then
          echo "Packaging session-management Lambda function..."
          cd one_l/agent_api/functions/knowledge_management/session_management
          zip -r /tmp/session-management.zip . -x "*.pyc" "__pycache__/*" "*.pyc" ".git/*"
          cd ../../../../../
          
          echo "Updating Lambda function code: $SESSION_MGMT_FUNCTION"
          aws lambda update-function-code \
            --function-name "$SESSION_MGMT_FUNCTION" \
            --zip-file fileb:///tmp/session-management.zip
          
          echo "Waiting for Lambda code update to complete..."
          aws lambda wait function-updated --function-name "$SESSION_MGMT_FUNCTION"
          
          # Update environment variables to ensure correct table names
          echo "Updating environment variables for session-management Lambda..."
          CURRENT_ENV=$(aws lambda get-function-configuration --function-name "$SESSION_MGMT_FUNCTION" --query 'Environment.Variables' --output json)
          
          # Get the correct table names from the stack
          ANALYSIS_TABLE=$(aws cloudformation describe-stacks --stack-name OneL-v2 --query 'Stacks[0].Outputs[?OutputKey==`AgentApiFunctionsStorageAnalysisTableNameA1B2C3D4`].OutputValue' --output text 2>/dev/null || echo "")
          SESSIONS_TABLE=$(aws cloudformation describe-stacks --stack-name OneL-v2 --query 'Stacks[0].Outputs[?OutputKey==`AgentApiFunctionsKnowledgeManagementSessionsTableNameE5F6G7H8`].OutputValue' --output text 2>/dev/null || echo "")
          
          # If we can't get from outputs, construct from stack name
          if [ -z "$ANALYSIS_TABLE" ]; then
            ANALYSIS_TABLE="OneL-v2-analysis-results"
          fi
          if [ -z "$SESSIONS_TABLE" ]; then
            SESSIONS_TABLE="OneL-v2-sessions"
          fi
          
          # Get bucket names from current environment or construct them
          KNOWLEDGE_BUCKET=$(echo $CURRENT_ENV | jq -r '.KNOWLEDGE_BUCKET // "onel-v2-knowledge-source"')
          USER_DOCS_BUCKET=$(echo $CURRENT_ENV | jq -r '.USER_DOCUMENTS_BUCKET // "onel-v2-user-documents"')
          AGENT_PROCESSING_BUCKET=$(echo $CURRENT_ENV | jq -r '.AGENT_PROCESSING_BUCKET // "onel-v2-agent-processing"')
          
          echo "Setting environment variables:"
          echo "  ANALYSIS_RESULTS_TABLE=$ANALYSIS_TABLE"
          echo "  SESSIONS_TABLE=$SESSIONS_TABLE"
          
          aws lambda update-function-configuration \
            --function-name "$SESSION_MGMT_FUNCTION" \
            --environment "Variables={KNOWLEDGE_BUCKET=$KNOWLEDGE_BUCKET,USER_DOCUMENTS_BUCKET=$USER_DOCS_BUCKET,AGENT_PROCESSING_BUCKET=$AGENT_PROCESSING_BUCKET,SESSIONS_TABLE=$SESSIONS_TABLE,ANALYSIS_RESULTS_TABLE=$ANALYSIS_TABLE,LOG_LEVEL=INFO}" \
            || echo "Warning: Failed to update environment variables, but code was updated"
          
          echo "Session-management Lambda function updated successfully"
        else
          echo "No session-management Lambda function found matching pattern"
        fi
        
        # List all OneL-v2 Lambda functions for verification
        echo "Listing all OneL-v2 Lambda functions:"
        aws lambda list-functions --query 'Functions[?contains(FunctionName, `OneL-v2`)].FunctionName' --output table

    - name: Get stack outputs
      id: stack-outputs
      run: |
        # First, list ALL outputs to see what's available
        echo "=== Listing ALL CloudFormation Outputs ==="
        aws cloudformation describe-stacks --stack-name OneL-v2 --query 'Stacks[0].Outputs[*].{Key:OutputKey,Value:OutputValue}' --output json > outputs.json
        aws cloudformation describe-stacks --stack-name OneL-v2 --query 'Stacks[0].Outputs[*].{Key:OutputKey,Value:OutputValue}' --output table
        
        # Extract values using actual output key names from V2 stack (not DV2!)
        # Get Cognito values
        COGNITO_DOMAIN=$(jq -r '.[] | select(.Key == "AuthorizationUserPoolDomainUrl533E5574") | .Value' outputs.json)
        USER_POOL_ID=$(jq -r '.[] | select(.Key == "AuthorizationUserPoolId47D46FEF") | .Value' outputs.json)
        USER_POOL_CLIENT_ID=$(jq -r '.[] | select(.Key == "AuthorizationUserPoolClientId24468C3D") | .Value' outputs.json)
        
        # Get API Gateway and WebSocket
        API_GATEWAY_URL=$(jq -r '.[] | select(.Key == "ApiGatewayMainApiUrl65B78421") | .Value' outputs.json)
        WEBSOCKET_URL=$(jq -r '.[] | select(.Key == "AgentApiFunctionsWebSocketWebSocketApiUrl63196B11") | .Value' outputs.json)
        
        # Get CloudFront URL and extract domain
        CLOUDFRONT_FULL_URL=$(jq -r '.[] | select(.Key == "UserInterfaceWebsiteUrlE8AF1FB6") | .Value' outputs.json)
        CLOUDFRONT_DOMAIN=$(echo $CLOUDFRONT_FULL_URL | sed 's|https://||')
        
        # Find CloudFront distribution ID from the domain
        DISTRIBUTION_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?DomainName=='${CLOUDFRONT_DOMAIN}'].Id" --output text)
        
        # Find the origin bucket for this CloudFront distribution
        echo "Finding bucket for CloudFront distribution: $DISTRIBUTION_ID"
        ORIGIN_BUCKET=$(aws cloudfront get-distribution --id $DISTRIBUTION_ID --query 'Distribution.DistributionConfig.Origins.Items[0].DomainName' --output text 2>/dev/null | sed 's/.s3.amazonaws.com//' | sed 's/.s3.us-east-1.amazonaws.com//' || echo "")
        
        if [ -n "$ORIGIN_BUCKET" ]; then
          WEBSITE_BUCKET=$ORIGIN_BUCKET
          echo "Found origin bucket: $WEBSITE_BUCKET"
        else
          # Fallback: try common bucket naming patterns
          echo "Could not auto-detect bucket, trying common patterns..."
          WEBSITE_BUCKET=$(aws s3 ls | grep -E "onel.*website|onel-v2-website" | awk '{print $3}' | head -1)
          if [ -z "$WEBSITE_BUCKET" ]; then
            echo "ERROR: Could not find website bucket!"
            exit 1
          fi
        fi
        
        # Debug output BEFORE setting environment variables
        echo "=== Debug: Retrieved Values ==="
        echo "WEBSITE_BUCKET: ${WEBSITE_BUCKET}"
        echo "CLOUDFRONT_DOMAIN: ${CLOUDFRONT_DOMAIN}"
        echo "COGNITO_DOMAIN: ${COGNITO_DOMAIN}"
        echo "API_GATEWAY_URL: ${API_GATEWAY_URL}"
        echo "WEBSOCKET_URL: ${WEBSOCKET_URL}"
        echo "USER_POOL_ID: ${USER_POOL_ID}"
        echo "USER_POOL_CLIENT_ID: ${USER_POOL_CLIENT_ID}"
        
        # Set environment variables
        echo "DISTRIBUTION_ID=${DISTRIBUTION_ID}" >> $GITHUB_ENV
        echo "CLOUDFRONT_DOMAIN=${CLOUDFRONT_DOMAIN}" >> $GITHUB_ENV
        echo "COGNITO_DOMAIN=${COGNITO_DOMAIN}" >> $GITHUB_ENV
        echo "API_GATEWAY_URL=${API_GATEWAY_URL}" >> $GITHUB_ENV
        echo "WEBSOCKET_URL=${WEBSOCKET_URL}" >> $GITHUB_ENV
        echo "WEBSITE_BUCKET=${WEBSITE_BUCKET}" >> $GITHUB_ENV
        echo "USER_POOL_ID=${USER_POOL_ID}" >> $GITHUB_ENV
        echo "USER_POOL_CLIENT_ID=${USER_POOL_CLIENT_ID}" >> $GITHUB_ENV
        
        echo "Stack outputs retrieved successfully"

    - name: Update Cognito OAuth settings
      run: |
        echo "Updating Cognito User Pool Client OAuth settings..."
        
        # Update with OAuth 2.0 settings enabled
        aws cognito-idp update-user-pool-client \
          --user-pool-id $USER_POOL_ID \
          --client-id $USER_POOL_CLIENT_ID \
          --callback-urls "https://${CLOUDFRONT_DOMAIN}" "https://${CLOUDFRONT_DOMAIN}/" \
          --logout-urls "https://${CLOUDFRONT_DOMAIN}" "https://${CLOUDFRONT_DOMAIN}/" \
          --supported-identity-providers "COGNITO" \
          --allowed-o-auth-flows "code" \
          --allowed-o-auth-scopes "openid" "email" "profile" \
          --allowed-o-auth-flows-user-pool-client \
          --explicit-auth-flows "ALLOW_USER_SRP_AUTH" "ALLOW_REFRESH_TOKEN_AUTH" \
          || (echo "Note: OAuth settings update may have failed, continuing..." && true)

    - name: Generate config.json
      run: |
        # Create config.json with proper Cognito domain format
        # Use the FULL domain URL for userPoolDomain field (not just the prefix)
        config_data=$(cat <<EOF
        {
          "apiGatewayUrl": "${API_GATEWAY_URL}",
          "userPoolId": "${USER_POOL_ID}",
          "userPoolClientId": "${USER_POOL_CLIENT_ID}",
          "userPoolDomain": "${COGNITO_DOMAIN}",
          "region": "us-east-1",
          "stackName": "OneL-v2",
          "knowledgeManagementUploadEndpointUrl": "${API_GATEWAY_URL}knowledge_management/upload",
          "knowledgeManagementRetrieveEndpointUrl": "${API_GATEWAY_URL}knowledge_management/retrieve",
          "knowledgeManagementDeleteEndpointUrl": "${API_GATEWAY_URL}knowledge_management/delete",
          "knowledgeManagementSyncEndpointUrl": "${API_GATEWAY_URL}knowledge_management/sync",
          "webSocketUrl": "${WEBSOCKET_URL}",
          "callbackUrl": "https://${CLOUDFRONT_DOMAIN}"
        }
        EOF
        )
        
        echo "$config_data" > config.json
        echo "Generated config.json:"
        cat config.json

    - name: Upload frontend build to S3
      run: |
        echo "Uploading React frontend build to S3 bucket: ${WEBSITE_BUCKET}"
        aws s3 sync one_l/user_interface/build/ s3://${WEBSITE_BUCKET}/ \
          --delete \
          --cache-control "public, max-age=31536000, immutable"
        aws s3 cp one_l/user_interface/build/index.html s3://${WEBSITE_BUCKET}/index.html \
          --cache-control "public, max-age=0, must-revalidate"
        echo "Frontend build uploaded successfully"

    - name: Upload config.json to S3
      run: |
        aws s3 cp config.json s3://${WEBSITE_BUCKET}/config.json --content-type application/json
        echo "Config.json uploaded successfully"

    - name: Invalidate CloudFront
      continue-on-error: true
      run: |
        echo "Attempting CloudFront invalidation (this may fail due to rate limits)..."
        aws cloudfront create-invalidation --distribution-id ${DISTRIBUTION_ID} --paths "/*" || echo "CloudFront invalidation skipped (rate limit exceeded or other error)"
        echo "CloudFront cache will expire naturally within 24 hours"

